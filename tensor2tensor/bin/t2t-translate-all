#!/usr/bin/env python
# coding=utf-8
# Copyright 2017 The Tensor2Tensor Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Translate a file with all checkpoints in a given directory.

t2t-decoder will be executed with these parameters:
--problems
--data_dir
--output_dir with the value of --model_dir
--decode_from_file with the value of --source
--decode_hparams with properly formated --beam_size and --alpha
--checkpoint_path automatically filled
--decode_to_file automatically filled
"""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import os
import time
from collections import namedtuple
import tensorflow as tf

flags = tf.flags

# t2t-translate-all specific options
flags.DEFINE_string("decoder_command", "t2t-decoder {params}", "Which command to execute instead t2t-decoder."
                    "{params} is replaced by the parameters. Useful e.g. for qsub wrapper.")
flags.DEFINE_string("model_dir", "", "Directory to load model checkpoints from.")
flags.DEFINE_string("source", None, "Path to the source-language file to be translated")
flags.DEFINE_string("translations_dir", "translations", "Where to store the translated files.")
flags.DEFINE_integer("min_steps", 0, "Ignore checkpoints with less steps.")
flags.DEFINE_integer("wait_secs", 0, "Wait upto N seconds for a new checkpoint, cf. save_checkpoints_secs.")
flags.DEFINE_integer("beam_size", 4, "Beam-search width.")
flags.DEFINE_float("alpha", 0.6, "Beam-search alpha.")

# options derived from t2t-decoder
flags.DEFINE_string("model", "transformer", "see t2t-decoder")
flags.DEFINE_string("t2t_usr_dir", None, "see t2t-decoder")
flags.DEFINE_string("data_dir", None, "see t2t-decoder")
flags.DEFINE_string("problems", None, "see t2t-decoder")
flags.DEFINE_string("hparams_set", "transformer_big_single_gpu", "see t2t-decoder")

Model = namedtuple('Model', 'filename time steps')


def read_checkpoints_list(model_dir, min_steps):
  try:
    models = [Model(x[:-6], os.path.getctime(x), int(x[:-6].rsplit('-')[-1]))
              for x in tf.gfile.Glob(os.path.join(model_dir, 'model.ckpt-*.index'))]
  except FileNotFoundError:
    # If the oldest file was deleted due to race conditions after Glob
    # and before getctime, try it once again. Models are being deleted
    # each save_checkpoints_secs, so there this time it should be OK.
    models = [Model(x[:-6], os.path.getctime(x), int(x[:-6].rsplit('-')[-1]))
              for x in tf.gfile.Glob(os.path.join(model_dir, 'model.ckpt-*.index'))]
  return sorted((x for x in models if x.steps > min_steps), key=lambda x: x.steps)

def main(_):
  FLAGS = flags.FLAGS
  tf.logging.set_verbosity(tf.logging.INFO)
  model_dir = os.path.expanduser(FLAGS.model_dir)
  translations_dir = os.path.expanduser(FLAGS.translations_dir)
  source = os.path.expanduser(FLAGS.source)
  os.makedirs(translations_dir, exist_ok=True)
  translated_base_file = os.path.join(translations_dir, FLAGS.problems)

  models = read_checkpoints_list(model_dir, FLAGS.min_steps)
  tf.logging.info("Found %d models with steps: %s" % (len(models), ", ".join(str(x.steps) for x in models)))

  exit_time = time.time() + FLAGS.wait_secs
  min_steps = FLAGS.min_steps
  while True:
    if not models and FLAGS.wait_secs:
      tf.logging.info('All checkpoints translated. Waiting till %s if a new checkpoint appears' % time.asctime(time.localtime(exit_time)))
      while True:
        time.sleep(10)
        models = read_checkpoints_list(model_dir, min_steps)
        if models or time.time() > exit_time:
          break
    if not models:
      return

    model = models.pop(0)
    exit_time, min_steps = model.time + FLAGS.wait_secs, model.steps
    tf.logging.info("Translating " + model.filename)
    out_file = translated_base_file + '-' + str(model.steps)
    if os.path.exists(out_file):
      tf.logging.info(out_file + " already exists, so skipping it.")
    else:
      tf.logging.info("Translating " + out_file)
      params = ("--t2t_usr_dir={FLAGS.t2t_usr_dir} --output_dir={model_dir} --data_dir={FLAGS.data_dir} "
         "--problems={FLAGS.problems} --decode_hparams=beam_size={FLAGS.beam_size},alpha={FLAGS.alpha} "
         "--model={FLAGS.model} --hparams_set={FLAGS.hparams_set} --checkpoint_path={model.filename} "
         "--decode_from_file={source} --decode_to_file={out_file}".format(**locals()))
      command = FLAGS.decoder_command.format(**locals())
      tf.logging.info("Running:\n" + command)
      os.system(command)

if __name__ == "__main__":
  tf.app.run()
