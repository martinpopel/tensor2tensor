#!/usr/bin/env python
# coding=utf-8
# Copyright 2017 The Tensor2Tensor Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Evaluate BLEU score for all checkpoints/translations in a given directory.

This script can be used in two ways.

To evaluate one already translated file:
`t2t-bleu --translation=my-wmt13.de --reference=wmt13_deen.de`

To evaluate all checkpoints in a given directory:
`t2t-bleu
  --model_dir=t2t_train
  --data_dir=t2t_data
  --translations_dir=my-translations
  --problems=translate_ende_wmt32k
  --hparams_set=transformer_big_single_gpu
  --source=wmt13_deen.en
  --reference=wmt13_deen.de`

In addition to the above-mentioned compulsory parameters,
there are optional parameters:

 * bleu_variant: cased (case-sensitive), uncased, both (default).
 * translations_dir: Where to store the translated files? Default="translations".
 * even_subdir: Where to store the even file? Default=$model_dir,
   i.e. TensorBoard will show it as the same run as the training, but it will warn
   about "more than one metagraph event per run". event_dir can be used e.g. if running
   this script several times with different `--decode_hparams="beam_size=$BEAM_SIZE,alpha=$ALPHA"`.
 * tag_suffix: Default="", so the tags will be BLEU_cased and BLEU_uncased. Again, tag_suffix
   can be used e.g. for different beam sizes if these should be plotted in different graphs.
 * min_steps: Don't evaluate checkpoints with less steps.
   Default=-1 means check the `last_evaluated_step.txt` file, which contains the number of steps
   of the last successfully evaluated checkpoint.
 * report_zero: Store BLEU=0 and guess its time based on flags.txt. Default=True.
   This is useful, so TensorBoard reports correct relative time for the remaining checkpoints.
   This flag is set to False if min_steps is > 0.
 * wait_secs: Wait upto N seconds for a new checkpoint. Default=0.
   This is useful for continuous evaluation of a running training,
   in which case this should be equal to save_checkpoints_secs plus some reserve.
"""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import os
import time
from collections import namedtuple
from tensor2tensor.utils import bleu_hook
import tensorflow as tf

flags = tf.flags
FLAGS = flags.FLAGS

flags.DEFINE_string("source", None, "Path to the source-language file to be translated")
flags.DEFINE_string("reference", None, "Path to the reference translation file")
flags.DEFINE_string("translation", None, "Path to the MT system translation file")
flags.DEFINE_string("translations_dir", None, "Directory with translated files to be evaulated.")
flags.DEFINE_string("model_dir", None, "Directory with checkpoints to be evaluated.")
flags.DEFINE_string("problems", None, "A basename of the translations files.")
flags.DEFINE_string("event_dir", "", "Where to store the event file. Default is model_dir.")

flags.DEFINE_string("bleu_variant", "both", "Possible values: cased(case-sensitive), uncased, both(default).")
flags.DEFINE_string("tag_suffix", "", "What to add to BLEU_cased and BLEU_uncased tags. Default=''.")
flags.DEFINE_integer("min_steps", -1, "Don't evaluate checkpoints with less steps.")
flags.DEFINE_integer("wait_secs", 0, "Wait upto N seconds for a new checkpoint, cf. save_checkpoints_secs.")
flags.DEFINE_bool("report_zero", None, "Store BLEU=0 and guess its time based on the oldest file in model_dir")
Model = namedtuple('Model', 'filename time steps')

def read_checkpoints_list(model_dir, min_steps):
  try:
    models = [Model(x[:-6], os.path.getmtime(x), int(x[:-6].rsplit('-')[-1]))
              for x in tf.gfile.Glob(os.path.join(model_dir, 'model.ckpt-*.index'))]
  except FileNotFoundError:
    # If the oldest file was deleted due to race conditions after Glob
    # and before getmtime, try it once again. Models are being deleted
    # each save_checkpoints_secs, so this time it should be OK.
    models = [Model(x[:-6], os.path.getmtime(x), int(x[:-6].rsplit('-')[-1]))
              for x in tf.gfile.Glob(os.path.join(model_dir, 'model.ckpt-*.index'))]
  return sorted((x for x in models if x.steps > min_steps), key=lambda x: x.steps)

def main(_):
  tf.logging.set_verbosity(tf.logging.INFO)
  if FLAGS.translation:
    if FLAGS.model_dir or FLAGS.translations_dir:
      raise ValueError('Cannot specify both --translation and --model_dir or --translations_dir.')
    if FLAGS.bleu_variant in ('uncased', 'both'):
      bleu = 100 * bleu_hook.bleu_wrapper(FLAGS.reference, FLAGS.translation, case_sensitive=False)
      print("BLEU_uncased = %6.2f" % bleu)
    if FLAGS.bleu_variant in ('cased', 'both'):
      bleu = 100 * bleu_hook.bleu_wrapper(FLAGS.reference, FLAGS.translation, case_sensitive=True)
      print("BLEU_cased = %6.2f" % bleu)
    return

  if not FLAGS.translation_dir:
    raise ValueError('Either --translation or --translations_dir must be specified.')
  translations_dir = os.path.expanduser(FLAGS.translations_dir)
  model_dir = os.path.expanduser(FLAGS.model_dir)
  if FLAGS.event_dir == '':
    FLAGS.event_dir = model_dir

  
  translated_base_file = os.path.join(translations_dir, FLAGS.problems)
  last_step_file = os.path.join(FLAGS.event_dir, 'last_evaluated_step.txt')
  if FLAGS.min_steps == -1:
    try:
      with open(last_step_file) as ls_file:
        FLAGS.min_steps = int(ls_file.read())
    except FileNotFoundError:
      FLAGS.min_steps = 0
  if FLAGS.report_zero is None:
    FLAGS.report_zero = FLAGS.min_steps == 0

  models = read_checkpoints_list(model_dir, FLAGS.min_steps)
  tf.logging.info("Found %d models with steps: %s" % (len(models), ", ".join(str(x.steps) for x in models)))

  writer = tf.summary.FileWriter(FLAGS.event_dir)
  if FLAGS.report_zero:
    all_files = (os.path.join(model_dir, f) for f in os.listdir(model_dir))
    start_time = min(os.path.getmtime(f) for f in all_files if os.path.isfile(f))
    values = []
    if FLAGS.bleu_variant in ('uncased', 'both'):
      values.append(tf.Summary.Value(tag='BLEU_uncased' + FLAGS.tag_suffix, simple_value=0))
    if FLAGS.bleu_variant in ('cased', 'both'):
      values.append(tf.Summary.Value(tag='BLEU_cased' + FLAGS.tag_suffix, simple_value=0))
    writer.add_event(tf.summary.Event(summary=tf.Summary(value=values), wall_time=start_time, step=0))

  exit_time = time.time() + FLAGS.wait_secs
  min_steps = FLAGS.min_steps
  while True:
    if not models and FLAGS.wait_secs:
      tf.logging.info('All checkpoints evaluated. Waiting till %s if a new checkpoint appears' % time.asctime(time.localtime(exit_time)))
      while True:
        time.sleep(10)
        models = read_checkpoints_list(model_dir, min_steps)
        if models or time.time() > exit_time:
          break
    if not models:
      return

    model = models.pop(0)
    exit_time, min_steps = model.time + FLAGS.wait_secs, model.steps
    tf.logging.info("Evaluating " + model.filename)
    out_file = translated_base_file + '-' + str(model.steps)
    if os.path.exists(out_file):
      tf.logging.info(out_file + " already exists, so reusing it.")
    else:
      translation_exit_time = time.time() + FLAGS.wait_secs
      tf.logging.info("Waiting till %s if %s will be created" % (time.asctime(time.localtime(translation_exit_time)), out_file))
      while True:
        time.sleep(10)
        if os.path.exists(out_file):
          break
        if time.time() > translation_exit_time:
          return
    values = []
    if FLAGS.bleu_variant in ('uncased', 'both'):
      bleu = 100 * bleu_hook.bleu_wrapper(FLAGS.reference, out_file, case_sensitive=False)
      values.append(tf.Summary.Value(tag='BLEU_uncased' + FLAGS.tag_suffix, simple_value=bleu))
      tf.logging.info("%s: BLEU_uncased = %6.2f" % (model.filename, bleu))
    if FLAGS.bleu_variant in ('cased', 'both'):
      bleu = 100 * bleu_hook.bleu_wrapper(FLAGS.reference, out_file, case_sensitive=True)
      values.append(tf.Summary.Value(tag='BLEU_cased' + FLAGS.tag_suffix, simple_value=bleu))
      tf.logging.info("%s: BLEU_cased = %6.2f" % (model.filename, bleu))
    writer.add_event(tf.summary.Event(summary=tf.Summary(value=values), wall_time=model.time, step=model.steps))
    writer.flush()
    with open(last_step_file, 'w') as ls_file:
      ls_file.write(str(model.steps) + '\n')


if __name__ == "__main__":
  tf.app.run()
